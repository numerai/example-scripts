{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqK_u9k-hMqE"
      },
      "source": [
        "# Model Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekw8Z93ljC3v",
        "outputId": "bdd16698-2ad0-4423-b090-c5ce55fe3053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoy_wT1rhMqF",
        "outputId": "e038b50f-1b61-4334-be62-28f4dc40a0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "plotnine 0.14.6 requires scipy<1.16.0,>=1.8.0, but you have scipy 1.16.0 which is incompatible.\n",
            "pylibcudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q --upgrade numerapi pandas pyarrow matplotlib lightgbm scikit-learn scipy cloudpickle==3.1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "13hdRk9ghMqI",
        "outputId": "d2274374-fd85-4189-f27b-d9d466cc63ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "v5.0/train.parquet: 2.37GB [01:28, 26.7MB/s]                            \n",
            "v5.0/features.json: 291kB [00:00, 1.45MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011829 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 688184, number of used features: 42\n",
            "[LightGBM] [Info] Start training from score 0.500008\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a959b0e2-3a84-4ffd-9205-035cdf0c5587\", \"example_model.pkl\", 6513463)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from numerapi import NumerAPI\n",
        "import pandas as pd\n",
        "import json\n",
        "napi = NumerAPI()\n",
        "\n",
        "# use one of the latest data versions\n",
        "DATA_VERSION = \"v5.0\"\n",
        "\n",
        "# Download data\n",
        "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
        "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
        "\n",
        "# Load data\n",
        "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
        "features = feature_metadata[\"feature_sets\"][\"small\"]\n",
        "# use \"medium\" or \"all\" for better performance. Requires more RAM.\n",
        "# features = feature_metadata[\"feature_sets\"][\"medium\"]\n",
        "# features = feature_metadata[\"feature_sets\"][\"all\"]\n",
        "train = pd.read_parquet(f\"{DATA_VERSION}/train.parquet\", columns=[\"era\"]+features+[\"target\"])\n",
        "\n",
        "# For better models, join train and validation data and train on all of it.\n",
        "# This would cause diagnostics to be misleading though.\n",
        "# napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
        "# validation = pd.read_parquet(f\"{DATA_VERSION}/validation.parquet\", columns=[\"era\"]+features+[\"target\"])\n",
        "# validation = validation[validation[\"data_type\"] == \"validation\"] # drop rows which don't have targets yet\n",
        "# train = pd.concat([train, validation])\n",
        "\n",
        "# Downsample for speed\n",
        "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]  # skip this step for better performance\n",
        "\n",
        "# Train model\n",
        "import lightgbm as lgb\n",
        "model = lgb.LGBMRegressor(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=5,\n",
        "    num_leaves=2**5-1,\n",
        "    colsample_bytree=0.1\n",
        ")\n",
        "# We've found the following \"deep\" parameters perform much better, but they require much more CPU and RAM\n",
        "# model = lgb.LGBMRegressor(\n",
        "#     n_estimators=30_000,\n",
        "#     learning_rate=0.001,\n",
        "#     max_depth=10,\n",
        "#     num_leaves=2**10,\n",
        "#     colsample_bytree=0.1,\n",
        "#     min_data_in_leaf=10000,\n",
        "# )\n",
        "model.fit(\n",
        "    train[features],\n",
        "    train[\"target\"]\n",
        ")\n",
        "\n",
        "# Define predict function\n",
        "def predict(\n",
        "    live_features: pd.DataFrame,\n",
        "    live_benchmark_models: pd.DataFrame\n",
        " ) -> pd.DataFrame:\n",
        "    live_predictions = model.predict(live_features[features])\n",
        "    submission = pd.Series(live_predictions, index=live_features.index)\n",
        "    return submission.to_frame(\"prediction\")\n",
        "\n",
        "# Pickle predict function\n",
        "import cloudpickle\n",
        "p = cloudpickle.dumps(predict)\n",
        "with open(\"example_model.pkl\", \"wb\") as f:\n",
        "    f.write(p)\n",
        "\n",
        "# Download file if running in Google Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('example_model.pkl')\n",
        "except:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}