{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqK_u9k-hMqE"
      },
      "source": [
        "# Model Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekw8Z93ljC3v",
        "outputId": "675ac893-5a46-4c6b-dc03-09438941d1fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoy_wT1rhMqF",
        "outputId": "4268fdb0-84d2-4502-97e4-e93a1440c8ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q numerapi pandas lightgbm cloudpickle==2.2.1 pyarrow scikit-learn scipy==1.10.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "13hdRk9ghMqI",
        "outputId": "857a4882-83e5-4a76-9b1e-57d6d822cc67"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_9cb9b662-7992-47b0-b787-453b845e7050\", \"predict_barebones.pkl\", 6572312)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from numerapi import NumerAPI\n",
        "import pandas as pd\n",
        "import json\n",
        "napi = NumerAPI()\n",
        "\n",
        "# use one of the latest data versions\n",
        "DATA_VERSION = \"v5.0\"\n",
        "\n",
        "# Download data\n",
        "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
        "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
        "\n",
        "# Load data\n",
        "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
        "features = feature_metadata[\"feature_sets\"][\"medium\"] # use \"all\" for better performance. Requires more RAM.\n",
        "train = pd.read_parquet(f\"{DATA_VERSION}/train.parquet\", columns=[\"era\"]+features+[\"target\"])\n",
        "\n",
        "# For better models, join train and validation data and train on all of it.\n",
        "# This would cause diagnostics to be misleading though.\n",
        "# napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
        "# validation = pd.read_parquet(f\"{DATA_VERSION}/validation.parquet\", columns=[\"era\"]+features+[\"target\"])\n",
        "# validation = validation[validation[\"data_type\"] == \"validation\"] # drop rows which don't have targets yet\n",
        "# train = pd.concat([train, validation])\n",
        "\n",
        "# Downsample for speed\n",
        "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]  # skip this step for better performance\n",
        "\n",
        "# Train model\n",
        "import lightgbm as lgb\n",
        "model = lgb.LGBMRegressor(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=5,\n",
        "    num_leaves=2**5-1,\n",
        "    colsample_bytree=0.1\n",
        ")\n",
        "# We've found the following \"deep\" parameters perform much better, but they require much more CPU and RAM\n",
        "# model = lgb.LGBMRegressor(\n",
        "#     n_estimators=30_000,\n",
        "#     learning_rate=0.001,\n",
        "#     max_depth=10,\n",
        "#     num_leaves=2**10,\n",
        "#     colsample_bytree=0.1\n",
        "#     min_data_in_leaf=10000,\n",
        "# )\n",
        "model.fit(\n",
        "    train[features],\n",
        "    train[\"target\"]\n",
        ")\n",
        "\n",
        "# Define predict function\n",
        "def predict(\n",
        "    live_features: pd.DataFrame,\n",
        "    live_benchmark_models: pd.DataFrame\n",
        " ) -> pd.DataFrame:\n",
        "    live_predictions = model.predict(live_features[features])\n",
        "    submission = pd.Series(live_predictions, index=live_features.index)\n",
        "    return submission.to_frame(\"prediction\")\n",
        "\n",
        "# Pickle predict function\n",
        "import cloudpickle\n",
        "p = cloudpickle.dumps(predict)\n",
        "with open(\"example_model.pkl\", \"wb\") as f:\n",
        "    f.write(p)\n",
        "\n",
        "# Download file if running in Google Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('example_model.pkl')\n",
        "except:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
